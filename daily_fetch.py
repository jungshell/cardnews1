"""ì¼ì¼ ìë™ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸"""
import os
import sys
import time
from datetime import datetime
from difflib import SequenceMatcher
from typing import Dict, List

import requests
from dotenv import load_dotenv

from daily_recommendations import load_daily_recommendations, save_daily_recommendations
from history_manager import add_crawl_history
from logger import logger
from naver_api import search_naver_news
from title_extractor import extract_full_title_from_url


# ê²€ìƒ‰ í‚¤ì›Œë“œ ëª©ë¡
# â€» ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ "ì¶©ë‚¨ì½˜í…ì¸ ", "ì¶©ë‚¨ ì½˜í…ì¸ "ëŠ” ì œì™¸
SEARCH_KEYWORDS = [
    "ì¶©ë‚¨ì½˜í…ì¸ ì§„í¥ì›",
    "ì¶©ì½˜ì§„",
    "ì²œì•ˆê·¸ë¦°ìŠ¤íƒ€íŠ¸ì—…íƒ€ìš´",
    "ê¹€ê³¡ë¯¸",
    "ì¶©ë‚¨ì½˜í…ì¸ ì½”ë¦¬ì•„ë©",
    "ì¶©ë‚¨ì½˜í…ì¸ ê¸°ì—…ì§€ì›ì„¼í„°",
    "ì¶©ë‚¨ê¸€ë¡œë²Œê²Œì„ì„¼í„°",
    "ì¶©ë‚¨ìŒì•…ì°½ì‘ì†Œ",
    "ì¶©ë‚¨ eìŠ¤í¬ì¸ ",
]


def _normalize_title(title: str) -> str:
    """
    ì œëª©ì„ ì •ê·œí™”í•˜ì—¬ ë¹„êµí•˜ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.
    
    Args:
        title: ì›ë³¸ ì œëª©
        
    Returns:
        ì •ê·œí™”ëœ ì œëª©
    """
    import re
    # ì†Œë¬¸ì ë³€í™˜
    normalized = title.lower()
    # ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°
    normalized = re.sub(r'[Â·\s\-Â·,ï¼Œ]', '', normalized)
    # ìœ ì‚¬í•œ í‘œí˜„ í†µì¼
    normalized = normalized.replace('ì„±ë£Œ', 'ì™„ë£Œ').replace('ë§ˆë¬´ë¦¬', 'ì™„ë£Œ')
    normalized = normalized.replace('ì„±ê³µì ', '').replace('ì„±ê³µ', '')
    normalized = normalized.replace('í•œêµ­ì²­ì†Œë…„ìœ¡ì„±íšŒ', 'ì²­ì†Œë…„ìœ¡ì„±íšŒ')
    normalized = normalized.replace('ì§€ì—­ì¸í”„ë¼ì—°ê³„', '').replace('ì¸í”„ë¼ì—°ê³„', '')
    # ì¶”ê°€ ì •ê·œí™”: ìœ ì‚¬ í‘œí˜„ í†µì¼
    normalized = normalized.replace('ìœµë³µí•©', 'ìœµí•©').replace('ìœµÂ·ë³µí•©', 'ìœµí•©')
    normalized = normalized.replace('ì½˜í…ì¸ ', 'ì½˜í…ì¸ ')
    return normalized


def remove_duplicate_articles(articles: List[Dict], similarity_threshold: float = 0.85) -> List[Dict]:
    """
    ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    
    Args:
        articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        similarity_threshold: ì œëª© ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ 0.85)
        
    Returns:
        ì¤‘ë³µì´ ì œê±°ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    if not articles:
        return []
    
    seen_links = set()
    seen_originallinks = set()
    unique_articles = []
    
    for article in articles:
        link = article.get("link", "")
        originallink = article.get("originallink", "")
        title = article.get("title", "").strip()
        
        # 1. ë§í¬ ì¤‘ë³µ ì²´í¬ (linkì™€ originallink ëª¨ë‘ ì²´í¬) - ê°€ì¥ í™•ì‹¤í•œ ì¤‘ë³µ ì²´í¬
        if link in seen_links:
            continue
        if originallink and originallink in seen_originallinks:
            continue
        
        # 2. ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ (ìœ íš¨í•˜ì§€ ì•Šì€ ê¸°ì‚¬)
        if len(title) < 10:
            continue
        
        # 3. ì •ê·œí™”ëœ ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
        normalized_title = _normalize_title(title)
        is_duplicate = False
        
        for existing in unique_articles:
            existing_title = existing.get("title", "").strip()
            if not existing_title:
                continue
            
            # ì •í™•íˆ ê°™ì€ ì œëª©ì´ë©´ ì¤‘ë³µ
            if title == existing_title:
                logger.debug(f"ì¤‘ë³µ ì œê±°: ì •í™•íˆ ê°™ì€ ì œëª© - '{title}'")
                is_duplicate = True
                break
            
            # ì •ê·œí™”ëœ ì œëª© ë¹„êµ
            existing_normalized = _normalize_title(existing_title)
            if normalized_title == existing_normalized:
                logger.info(f"ì¤‘ë³µ ì œê±°: ì •ê·œí™” í›„ ë™ì¼ - '{title}' vs '{existing_title}'")
                is_duplicate = True
                break
            
            # ìœ ì‚¬ë„ ì²´í¬ (ì •ê·œí™”ëœ ì œëª©ìœ¼ë¡œ)
            similarity = SequenceMatcher(None, normalized_title, existing_normalized).ratio()
            if similarity >= similarity_threshold:
                logger.info(f"ì¤‘ë³µ ì œê±°: ìœ ì‚¬ë„ {similarity:.2f} - '{title}' vs '{existing_title}'")
                is_duplicate = True
                break
        
        if is_duplicate:
            continue
        
        # 4. originallinkê°€ ê°™ìœ¼ë©´ ì¤‘ë³µ (ê°€ì¥ í™•ì‹¤í•œ ì¤‘ë³µ ì²´í¬)
        if originallink:
            for existing in unique_articles:
                existing_originallink = existing.get("originallink", "")
                if existing_originallink and originallink == existing_originallink:
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            seen_links.add(link)
            if originallink:
                seen_originallinks.add(originallink)
            unique_articles.append(article)
    
    return unique_articles


def calculate_relevance_score(article: Dict, keywords: List[str]) -> float:
    """
    ê¸°ì‚¬ì˜ ê´€ë ¨ë„ ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (10ì  ë§Œì )
    
    Args:
        article: ê¸°ì‚¬ ì •ë³´
        keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ê´€ë ¨ë„ ì ìˆ˜ (0.0 ~ 10.0, ë†’ì„ìˆ˜ë¡ ê´€ë ¨ë„ ë†’ìŒ)
    """
    score = 0.0
    title = article.get("title", "").lower()
    description = article.get("description", "").lower()
    
    # ì£¼ìš” í‚¤ì›Œë“œ (íšŒì‚¬ëª…) - ë†’ì€ ê°€ì¤‘ì¹˜
    main_keywords = ["ì¶©ë‚¨ì½˜í…ì¸ ì§„í¥ì›", "ì¶©ì½˜ì§„"]
    other_keywords = [k for k in keywords if k not in main_keywords]
    
    # 1. ì œëª© ë§¤ì¹­ ì ìˆ˜ (ìµœëŒ€ 5ì )
    title_score = 0.0
    for keyword in main_keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in title:
            title_score += 2.5  # ì£¼ìš” í‚¤ì›Œë“œ: ê° 2.5ì 
    
    for keyword in other_keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in title:
            title_score += 0.3  # ê¸°íƒ€ í‚¤ì›Œë“œ: ê° 0.3ì 
    
    title_score = min(title_score, 5.0)  # ìµœëŒ€ 5ì 
    score += title_score
    
    # 2. ì„¤ëª… ë§¤ì¹­ ì ìˆ˜ (ìµœëŒ€ 3ì )
    desc_score = 0.0
    for keyword in main_keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in description:
            desc_score += 1.5  # ì£¼ìš” í‚¤ì›Œë“œ: ê° 1.5ì 
    
    for keyword in other_keywords:
        keyword_lower = keyword.lower()
        if keyword_lower in description:
            desc_score += 0.2  # ê¸°íƒ€ í‚¤ì›Œë“œ: ê° 0.2ì 
    
    desc_score = min(desc_score, 3.0)  # ìµœëŒ€ 3ì 
    score += desc_score
    
    # 3. ìµœê·¼ ê¸°ì‚¬ ë³´ë„ˆìŠ¤ ì ìˆ˜ (ìµœëŒ€ 2ì )
    pub_date = article.get("pubDate", "")
    if pub_date:
        try:
            # ISO í˜•ì‹ íŒŒì‹± (ì˜ˆ: "2024-12-24T09:00:00+09:00")
            date_str = pub_date.split("T")[0]
            article_date = datetime.strptime(date_str, "%Y-%m-%d")
            today = datetime.now()
            days_diff = (today - article_date).days
            
            # ìµœê·¼ 4ì¼ ì´ë‚´ë©´ ë³´ë„ˆìŠ¤ ì ìˆ˜ (4ì¼ ì „: 0.5ì , ë‹¹ì¼: 2ì )
            if days_diff <= 4:
                bonus = 2.0 - (days_diff * 0.375)  # 0ì¼: 2ì , 1ì¼: 1.625ì , 2ì¼: 1.25ì , 3ì¼: 0.875ì , 4ì¼: 0.5ì 
                score += bonus
        except Exception:
            pass
    
    # ìµœëŒ€ 10ì ìœ¼ë¡œ ì œí•œ
    return min(score, 10.0)


def fetch_daily_recommendations() -> List[Dict]:
    """
    ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³ , ì¤‘ë³µ ì œê±° ë° ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° í›„ ì¶”ì²œ ê¸°ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Returns:
        ì¶”ì²œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ê´€ë ¨ë„ ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)
    """
    all_articles = []
    
    logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {len(SEARCH_KEYWORDS)}ê°œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰")
    
    # ê´€ë ¨ë„ìˆœê³¼ ë‚ ì§œìˆœ ëª¨ë‘ ê²€ìƒ‰í•˜ì—¬ ë” ë§ì€ ê¸°ì‚¬ ìˆ˜ì§‘
    for keyword in SEARCH_KEYWORDS:
        logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì¤‘: {keyword}")
        # ë‚ ì§œìˆœ ê²€ìƒ‰ (ìµœì‹  ê¸°ì‚¬)
        articles_date = search_naver_news(keyword, display=100, sort="date")
        all_articles.extend(articles_date)
        logger.info(f"í‚¤ì›Œë“œ '{keyword}' (ë‚ ì§œìˆœ): {len(articles_date)}ê°œ ê¸°ì‚¬ ë°œê²¬")
        
        # ê´€ë ¨ë„ìˆœ ê²€ìƒ‰ (ê´€ë ¨ë„ ë†’ì€ ê¸°ì‚¬) - ì¤‘ë³µì´ì§€ë§Œ ë‹¤ë¥¸ ê¸°ì‚¬ë„ í¬í•¨ë  ìˆ˜ ìˆìŒ
        articles_sim = search_naver_news(keyword, display=100, sort="sim")
        all_articles.extend(articles_sim)
        logger.info(f"í‚¤ì›Œë“œ '{keyword}' (ê´€ë ¨ë„ìˆœ): {len(articles_sim)}ê°œ ê¸°ì‚¬ ë°œê²¬")
        
        # API í˜¸ì¶œ ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì§§ì€ ëŒ€ê¸°
        time.sleep(0.1)
    
    logger.info(f"ì¤‘ë³µ ì œê±° ì „: {len(all_articles)}ê°œ ê¸°ì‚¬")
    unique_articles = remove_duplicate_articles(all_articles)
    logger.info(f"ì¤‘ë³µ ì œê±° í›„: {len(unique_articles)}ê°œ ê¸°ì‚¬ (ì œê±°: {len(all_articles) - len(unique_articles)}ê°œ)")
    
    # ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° (ì „ì²´ ì œëª© ì¶”ì¶œ ì „ì— ë¨¼ì € ì ìˆ˜ ê³„ì‚°)
    logger.info("ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚° ì¤‘...")
    scored_articles = []
    for idx, article in enumerate(unique_articles, 1):
        score = calculate_relevance_score(article, SEARCH_KEYWORDS)
        # 10ì  ë§Œì ìœ¼ë¡œ ì œí•œ (í˜¹ì‹œ ëª¨ë¥¼ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
        score = min(score, 10.0)
        article["relevance_score"] = score
        scored_articles.append(article)
    
    # ê´€ë ¨ë„ ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    scored_articles.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # ìƒìœ„ ê¸°ì‚¬ë§Œ ì„ ì • (ìµœëŒ€ 50ê°œ)
    top_articles = scored_articles[:50]
    
    # ìƒìœ„ ê¸°ì‚¬ì—ë§Œ ì „ì²´ ì œëª© ì¶”ì¶œ (í¬ë¡¤ë§ ì‹œê°„ ë‹¨ì¶•)
    # ìƒìœ„ 20ê°œë§Œ ì „ì²´ ì œëª© ì¶”ì¶œ (ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ ì œëª© ì‚¬ìš©)
    logger.info(f"ìƒìœ„ {min(20, len(top_articles))}ê°œ ê¸°ì‚¬ì˜ ì „ì²´ ì œëª© ì¶”ì¶œ ì¤‘...")
    for idx, article in enumerate(top_articles[:20], 1):
        original_link = article.get("originallink") or article.get("link", "")
        if original_link:
            logger.info(f"[{idx}/{min(20, len(top_articles))}] ì „ì²´ ì œëª© ì¶”ì¶œ ì¤‘: {original_link[:50]}...")
            full_title = extract_full_title_from_url(original_link)
            if full_title:
                article["full_title"] = full_title.strip()
                logger.info(f"  â†’ ì „ì²´ ì œëª© ì¶”ì¶œ ì„±ê³µ: {full_title[:50]}...")
            else:
                logger.warning(f"  â†’ ì „ì²´ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ì¡´ ì œëª© ì‚¬ìš©")
    
    logger.info(f"ì™„ë£Œ: ìƒìœ„ {len(top_articles)}ê°œ ê¸°ì‚¬ ì„ ì •")
    
    return top_articles


def _clean_html_tags(text: str) -> str:
    """HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    import re
    if not text:
        return ""
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = text.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&nbsp;', ' ').replace('&#39;', "'").replace('&apos;', "'")
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def _format_date(pub_date: str) -> str:
    """ë‚ ì§œë¥¼ í•œêµ­ì–´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not pub_date:
        return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
    
    try:
        # ISO í˜•ì‹ íŒŒì‹± (ì˜ˆ: "2025-12-30T10:30:00+09:00")
        if 'T' in pub_date:
            date_str = pub_date.split('T')[0]
            dt = datetime.strptime(date_str, "%Y-%m-%d")
        else:
            dt = datetime.strptime(pub_date, "%Y-%m-%d")
        
        # í•œêµ­ì–´ í˜•ì‹: "2025.12.30 (í™”)"
        weekdays = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"]
        weekday = weekdays[dt.weekday()]
        return f"{dt.strftime('%Y.%m.%d')} ({weekday})"
    except Exception:
        return pub_date


def send_slack_notification(articles: List[Dict]) -> bool:
    """
    Slackìœ¼ë¡œ ì¼ì¼ ì¶”ì²œ ê¸°ì‚¬ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
    
    Args:
        articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ 5ê°œë§Œ ì „ì†¡)
        
    Returns:
        ì „ì†¡ ì„±ê³µ ì—¬ë¶€
    """
    webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    if not webhook_url:
        logger.warning("SLACK_WEBHOOK_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•„ Slack ì•Œë¦¼ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    
    # ìƒìœ„ 5ê°œë§Œ ì „ì†¡
    top_5 = articles[:5]
    
    # Block Kit í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ êµ¬ì„±
    blocks = [
        {
            "type": "header",
            "text": {
                "type": "plain_text",
                "text": "ğŸ“° ì˜¤ëŠ˜ì˜ ì¶”ì²œ ê¸°ì‚¬",
            },
        },
        {
            "type": "divider",
        },
    ]
    
    for idx, article in enumerate(top_5, 1):
        # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ ë° HTML íƒœê·¸ ì œê±°
        title = _clean_html_tags(article.get("title", ""))
        description = _clean_html_tags(article.get("description", ""))
        link = article.get("link", "")
        score = article.get("relevance_score", 0)
        pub_date = article.get("pubDate", "")
        
        # ë‚ ì§œ í¬ë§·íŒ…
        formatted_date = _format_date(pub_date)
        
        # ìš”ì•½ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìºì‹œì—ì„œ)
        article_id = link or title
        try:
            from cache_manager import get_cached_summary
            summary = get_cached_summary(article_id)
        except Exception:
            summary = None
        
        # ê¸°ì‚¬ ì œëª© (ì œëª©ë§Œ ê°•ì¡°)
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": f"*{idx}. {title}*",
            },
        })
        
        # ë©”íƒ€ ì •ë³´ (ë‚ ì§œ, ê´€ë ¨ë„ ì ìˆ˜)
        blocks.append({
            "type": "context",
            "elements": [
                {
                    "type": "mrkdwn",
                    "text": f"ğŸ“… {formatted_date}  |  ğŸ“Š ê´€ë ¨ë„: {score:.1f}/10ì ",
                },
            ],
        })
        
        # ê¸°ì‚¬ ì„¤ëª… (ê°„ëµ)
        if description:
            desc_short = description[:150] + "..." if len(description) > 150 else description
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": desc_short,
                },
            })
        
        # ìš”ì•½ì´ ìˆìœ¼ë©´ í‘œì‹œ
        if summary:
            # ìš”ì•½ ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°)
            summary_short = summary[:200] + "..." if len(summary) > 200 else summary
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": f"*ğŸ“„ ìš”ì•½:*\n{summary_short}",
                },
            })
        
        # ë²„íŠ¼ë“¤
        buttons = []
        if link:
            buttons.append({
                "type": "button",
                "text": {
                    "type": "plain_text",
                    "text": "ğŸ”— ê¸°ì‚¬ ë³´ê¸°",
                },
                "url": link,
                "action_id": f"view_article_{idx}",
            })
        
        # ì¹´ë“œë‰´ìŠ¤ ìƒì„± ë²„íŠ¼
        slack_app_url = os.getenv("SLACK_APP_URL")
        if slack_app_url:
            # Interactive ë²„íŠ¼ (Slack App ì„œë²„ë¡œ ìš”ì²­)
            buttons.append({
                "type": "button",
                "text": {
                    "type": "plain_text",
                    "text": "ğŸ“„ ìš”ì•½ ë³´ê¸°",
                },
                "action_id": f"view_summary_{idx}",
                "value": str(idx),
            })
            
            buttons.append({
                "type": "button",
                "text": {
                    "type": "plain_text",
                    "text": "ğŸ“ ì¹´ë“œë‰´ìŠ¤ ìƒì„±",
                },
                "action_id": f"create_cardnews_{idx}",
                "value": str(idx),
            })
        else:
            # ì¼ë°˜ URL ë²„íŠ¼ (Streamlit ì•± ë§í¬)
            streamlit_base_url = os.getenv("STREAMLIT_APP_URL", "https://cardnews1-hd646zyxsbzawjaibtjgar.streamlit.app")
            import urllib.parse
            streamlit_url = f"{streamlit_base_url}?article_url={urllib.parse.quote(link)}" if link else streamlit_base_url
            buttons.append({
                "type": "button",
                "text": {
                    "type": "plain_text",
                    "text": "ğŸ“ ì¹´ë“œë‰´ìŠ¤ ìƒì„±",
                },
                "url": streamlit_url,
                "action_id": f"create_cardnews_{idx}",
            })
        
        if buttons:
            blocks.append({
                "type": "actions",
                "elements": buttons,
            })
        
        # êµ¬ë¶„ì„ 
        if idx < len(top_5):
            blocks.append({"type": "divider"})
    
    payload = {"blocks": blocks}
    
    try:
        resp = requests.post(webhook_url, json=payload, timeout=10)
        resp.raise_for_status()
        logger.info(f"Slack ì•Œë¦¼ ì „ì†¡ ì™„ë£Œ: {len(top_5)}ê°œ ê¸°ì‚¬")
        return True
    except Exception as e:
        logger.error(f"Slack ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    env_path = os.path.join(os.path.dirname(__file__), ".env")
    if os.path.exists(env_path):
        load_dotenv(env_path)
    
    # stdoutì—ë„ ì¶œë ¥ (Streamlitì—ì„œ subprocess ë¡œê·¸ë¥¼ ë³´ê¸° ìœ„í•´)
    import sys
    def log_and_print(message, level="info"):
        """loggerì™€ stdout ëª¨ë‘ì— ì¶œë ¥"""
        if level == "info":
            logger.info(message)
            print(message, flush=True)
        elif level == "warning":
            logger.warning(message)
            print(f"[ê²½ê³ ] {message}", flush=True)
        elif level == "error":
            logger.error(message)
            print(f"[ì˜¤ë¥˜] {message}", flush=True)
        else:
            logger.info(message)
            print(message, flush=True)
    
    # ëª…ë ¹ì¤„ ì¸ì í™•ì¸
    slack_only = len(sys.argv) > 1 and sys.argv[1] == "slack-only"
    
    if slack_only:
        # Slack ì•Œë¦¼ë§Œ ì „ì†¡
        log_and_print("Slack ì•Œë¦¼ë§Œ ì „ì†¡ ëª¨ë“œ")
        articles = load_daily_recommendations()
        if articles:
            send_slack_notification(articles)
        else:
            log_and_print("daily_recommendations.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.", "error")
    else:
        # í¬ë¡¤ë§ ì‹¤í–‰
        log_and_print("=" * 60)
        log_and_print("ì¼ì¼ ìë™ í¬ë¡¤ë§ ì‹œì‘")
        log_and_print("=" * 60)
        
        articles = fetch_daily_recommendations()
        
        if articles:
            # daily_recommendations.jsonì— ì €ì¥
            save_daily_recommendations(articles)
            log_and_print(f"ì €ì¥ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ daily_recommendations.jsonì— ì €ì¥")
            
            # í¬ë¡¤ë§ ê¸°ë¡ ì €ì¥
            add_crawl_history("ì¼ì¼ ìë™ í¬ë¡¤ë§", len(articles))
            
            # Slack ì•Œë¦¼ ì „ì†¡
            send_slack_notification(articles)
        else:
            log_and_print("ì¶”ì²œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "warning")
    
    log_and_print("=" * 60)
    log_and_print("ì™„ë£Œ")
    log_and_print("=" * 60)


if __name__ == "__main__":
    main()

